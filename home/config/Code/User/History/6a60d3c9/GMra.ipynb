{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/griffin/git/griffinht.com/notebooks/tailscale_all\n"
     ]
    }
   ],
   "source": [
    "%cd tailscale_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pod.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pod.yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: tailscale-state\n",
    "spec:\n",
    "  accessModes:\n",
    "    # read/write from a single node\n",
    "    # https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\n",
    "    - ReadWriteOnce\n",
    "  # todo idk where the docs are for this - i just guessed\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Mi\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: caddy\n",
    "spec:\n",
    "    containers:\n",
    "      - name: iperf3\n",
    "        image: networkstatic/iperf3\n",
    "        args:\n",
    "          - --server\n",
    "      - name: caddy\n",
    "        image: caddy/caddy\n",
    "      - name: tailscale\n",
    "        image: ghcr.io/tailscale/tailscale:latest\n",
    "        env:\n",
    "          # disable Kubernetes secrets and use OAuth login instead\n",
    "          - name: TS_KUBE_SECRET\n",
    "            value: \"\"\n",
    "          - name: TS_STATE_DIR\n",
    "            value: \"/var/lib/tailscale\"\n",
    "        volumeMounts:\n",
    "          - name: tailscale-state\n",
    "            mountPath: \"/var/lib/tailscale\"\n",
    "    volumes:\n",
    "      - name: tailscale-state\n",
    "        persistentVolumeClaim:\n",
    "          claimName: tailscale-state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/tailscale-state created\n",
      "pod/caddy created\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "kubectl apply -f pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Tip: If your pod is taking a while to come up, check out `kubectl describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             caddy\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             gk3-my-cluster-pool-3-31f1207c-nfrt/10.142.0.7\n",
      "Start Time:       Sun, 23 Jun 2024 18:25:21 -0400\n",
      "Labels:           <none>\n",
      "Annotations:      autopilot.gke.io/resource-adjustment:\n",
      "                    {\"input\":{\"containers\":[{\"name\":\"iperf3\"},{\"name\":\"caddy\"},{\"name\":\"tailscale\"}]},\"output\":{\"containers\":[{\"limits\":{\"cpu\":\"500m\",\"ephemer...\n",
      "                  autopilot.gke.io/warden-version: 2.9.37\n",
      "Status:           Running\n",
      "SeccompProfile:   RuntimeDefault\n",
      "IP:               10.15.0.133\n",
      "IPs:\n",
      "  IP:  10.15.0.133\n",
      "Containers:\n",
      "  iperf3:\n",
      "    Container ID:   containerd://b2bc732be4b3a938eea50dae2bb9e5dbdf8e14099c6fcf5d9a8c94dcaa501dd6\n",
      "    Image:          networkstatic/iperf3\n",
      "    Image ID:       docker.io/networkstatic/iperf3@sha256:148ec0389185caa366802a996ccded1bcde221a257f3eb44f291a7ac765f4ed4\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Waiting\n",
      "      Reason:       CrashLoopBackOff\n",
      "    Last State:     Terminated\n",
      "      Reason:       Error\n",
      "      Exit Code:    1\n",
      "      Started:      Sun, 23 Jun 2024 18:26:34 -0400\n",
      "      Finished:     Sun, 23 Jun 2024 18:26:34 -0400\n",
      "    Ready:          False\n",
      "    Restart Count:  3\n",
      "    Limits:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Requests:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Environment:          <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmdtl (ro)\n",
      "  caddy:\n",
      "    Container ID:   containerd://5f57020a636e26b015f2254443e358615f84b59b2cd4cb96b68dd149306ac94a\n",
      "    Image:          caddy/caddy\n",
      "    Image ID:       docker.io/caddy/caddy@sha256:93bce93e51020d5d1c8ca4ab6b0b73af0045f0e97c272784497e02b4ad30ad20\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Sun, 23 Jun 2024 18:25:43 -0400\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Requests:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Environment:          <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmdtl (ro)\n",
      "  tailscale:\n",
      "    Container ID:   containerd://e066c4616da630a2d3051c757ad722d2b7257c4d7264e13e7a94c771093994c8\n",
      "    Image:          ghcr.io/tailscale/tailscale:latest\n",
      "    Image ID:       ghcr.io/tailscale/tailscale@sha256:a0d1a9ed2abfacf905c0e3423aea00181064162e548f875f422a03924b9cc5c4\n",
      "    Port:           <none>\n",
      "    Host Port:      <none>\n",
      "    State:          Running\n",
      "      Started:      Sun, 23 Jun 2024 18:25:45 -0400\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Requests:\n",
      "      cpu:                500m\n",
      "      ephemeral-storage:  1Gi\n",
      "      memory:             2Gi\n",
      "    Environment:\n",
      "      TS_KUBE_SECRET:  \n",
      "      TS_STATE_DIR:    /var/lib/tailscale\n",
      "    Mounts:\n",
      "      /var/lib/tailscale from tailscale-state (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hmdtl (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       False \n",
      "  ContainersReady             False \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  tailscale-state:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  tailscale-state\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-hmdtl:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Guaranteed\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 kubernetes.io/arch=amd64:NoSchedule\n",
      "                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type     Reason                  Age                From                                   Message\n",
      "  ----     ------                  ----               ----                                   -------\n",
      "  Warning  FailedScheduling        2m34s              gke.io/optimize-utilization-scheduler  0/2 nodes are available: 2 Insufficient cpu, 2 Insufficient memory. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.\n",
      "  Normal   TriggeredScaleUp        2m29s              cluster-autoscaler                     pod triggered scale-up: [{https://www.googleapis.com/compute/v1/projects/griffinht-cloudlab/zones/us-east1-d/instanceGroups/gk3-my-cluster-pool-3-31f1207c-grp 0->1 (max: 1000)}]\n",
      "  Normal   Scheduled               85s                gke.io/optimize-utilization-scheduler  Successfully assigned default/caddy to gk3-my-cluster-pool-3-31f1207c-nfrt\n",
      "  Normal   SuccessfulAttachVolume  80s                attachdetach-controller                AttachVolume.Attach succeeded for volume \"pvc-9e79f40c-c724-473b-afa3-87d60242de69\"\n",
      "  Normal   Pulling                 69s                kubelet                                Pulling image \"caddy/caddy\"\n",
      "  Normal   Pulled                  69s                kubelet                                Successfully pulled image \"networkstatic/iperf3\" in 7.322s (7.322s including waiting)\n",
      "  Normal   Pulling                 63s                kubelet                                Pulling image \"ghcr.io/tailscale/tailscale:latest\"\n",
      "  Normal   Pulled                  63s                kubelet                                Successfully pulled image \"caddy/caddy\" in 5.456s (5.456s including waiting)\n",
      "  Normal   Created                 63s                kubelet                                Created container caddy\n",
      "  Normal   Started                 63s                kubelet                                Started container caddy\n",
      "  Normal   Pulled                  61s                kubelet                                Successfully pulled image \"ghcr.io/tailscale/tailscale:latest\" in 1.87s (1.87s including waiting)\n",
      "  Normal   Created                 61s                kubelet                                Created container tailscale\n",
      "  Normal   Started                 61s                kubelet                                Started container tailscale\n",
      "  Normal   Pulled                  60s                kubelet                                Successfully pulled image \"networkstatic/iperf3\" in 413ms (413ms including waiting)\n",
      "  Normal   Created                 42s (x3 over 69s)  kubelet                                Created container iperf3\n",
      "  Normal   Pulled                  42s                kubelet                                Successfully pulled image \"networkstatic/iperf3\" in 442ms (442ms including waiting)\n",
      "  Normal   Started                 41s (x3 over 69s)  kubelet                                Started container iperf3\n",
      "  Warning  BackOff                 28s (x4 over 58s)  kubelet                                Back-off restarting failed container iperf3 in pod caddy_default(55140569-cf19-4f6f-9b8a-b751d25a1a4c)\n",
      "  Normal   Pulling                 13s (x4 over 76s)  kubelet                                Pulling image \"networkstatic/iperf3\"\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "kubectl describe pod caddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host caddy, port 5201\n",
      "[  5] local 100.99.123.66 port 39614 connected to 100.123.205.111 port 5201\n",
      "[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n",
      "[  5]   0.00-1.00   sec  11.8 MBytes  98.5 Mbits/sec  1608    186 KBytes       \n",
      "[  5]   1.00-2.00   sec  8.12 MBytes  68.2 Mbits/sec    0    211 KBytes       \n",
      "[  5]   2.00-3.00   sec  8.25 MBytes  69.2 Mbits/sec    0    239 KBytes       \n",
      "[  5]   3.00-4.00   sec  9.62 MBytes  80.7 Mbits/sec    5    199 KBytes       \n",
      "[  5]   4.00-5.00   sec  6.88 MBytes  57.7 Mbits/sec   11    159 KBytes       \n",
      "[  5]   5.00-6.00   sec  8.25 MBytes  69.2 Mbits/sec    0    187 KBytes       \n",
      "[  5]   6.00-7.00   sec  8.12 MBytes  68.2 Mbits/sec    0    216 KBytes       \n",
      "[  5]   7.00-8.00   sec  9.62 MBytes  80.7 Mbits/sec   25    174 KBytes       \n",
      "[  5]   8.00-9.00   sec  8.12 MBytes  68.2 Mbits/sec    0    204 KBytes       \n",
      "[  5]   9.00-10.00  sec  8.25 MBytes  69.2 Mbits/sec    0    227 KBytes       \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[ ID] Interval           Transfer     Bitrate         Retr\n",
      "[  5]   0.00-10.00  sec  87.0 MBytes  73.0 Mbits/sec  1649             sender\n",
      "[  5]   0.00-10.03  sec  84.4 MBytes  70.6 Mbits/sec                  receiver\n",
      "\n",
      "iperf Done.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "iperf3 -c caddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "It works! We added `iperf3` to our pod, and connected to it via Tailscale.\n",
    "\n",
    "I want to separate the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pod.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pod.yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: tailscale-state\n",
    "spec:\n",
    "  accessModes:\n",
    "    # read/write from a single node\n",
    "    # https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\n",
    "    - ReadWriteOnce\n",
    "  # todo idk where the docs are for this - i just guessed\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Mi\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: iperf3\n",
    "spec:\n",
    "    containers:\n",
    "      - name: iperf3\n",
    "        image: networkstatic/iperf3\n",
    "        args:\n",
    "          - --server\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: caddy\n",
    "spec:\n",
    "    containers:\n",
    "      - name: caddy\n",
    "        image: caddy/caddy\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: tailscale\n",
    "spec:\n",
    "    containers:\n",
    "      - name: tailscale\n",
    "        image: ghcr.io/tailscale/tailscale:latest\n",
    "        env:\n",
    "          # disable Kubernetes secrets and use OAuth login instead\n",
    "          - name: TS_KUBE_SECRET\n",
    "            value: \"\"\n",
    "          - name: TS_STATE_DIR\n",
    "            value: \"/var/lib/tailscale\"\n",
    "        volumeMounts:\n",
    "          - name: tailscale-state\n",
    "            mountPath: \"/var/lib/tailscale\"\n",
    "    volumes:\n",
    "      - name: tailscale-state\n",
    "        persistentVolumeClaim:\n",
    "          claimName: tailscale-state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/tailscale-state unchanged\n",
      "pod/iperf3 unchanged\n",
      "pod/caddy unchanged\n",
      "pod/tailscale configured\n"
     ]
    }
   ],
   "source": [
    "! kubectl apply -f pod.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.113.140.69  tailscale            griffinht@   linux   -\n"
     ]
    }
   ],
   "source": [
    "! tailscale status | grep tailscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Great! Now Tailscale shows up in its own pod as the \"tailscale\" hostname. We could also change the hostname if we wanted to, but I'll leave it like this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (7) Failed to connect to tailscale port 80 after 95 ms: Couldn't connect to server\n"
     ]
    }
   ],
   "source": [
    "! curl tailscale:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "As we saw in the Podman pod lab todo. Caddy and Iperf3 are no longer in the same pod as Tailscale, and can't be reached via localhost. Let's try to reach Caddy from Tailscale, like we would in Docker Compose. todo docker compose lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: bad address 'caddy:80'\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "! kubectl exec tailscale -- wget caddy:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        READY   STATUS    RESTARTS   AGE   IP            NODE                 NOMINATED NODE   READINESS GATES\n",
      "caddy       1/1     Running   0          13h   10.244.0.11   kind-control-plane   <none>           <none>\n",
      "iperf3      1/1     Running   0          13h   10.244.0.10   kind-control-plane   <none>           <none>\n",
      "tailscale   1/1     Running   0          27m   10.244.0.14   kind-control-plane   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -o wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 10.244.0.11:80 (10.244.0.11:80)\n",
      "saving to 'index.html'\n",
      "index.html           100% |********************************| 12256  0:00:00 ETA\n",
      "'index.html' saved\n"
     ]
    }
   ],
   "source": [
    "! kubectl exec tailscale -- wget 10.244.0.11:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Found it! Caddy is still accessible from the Tailscale pod, but doesn't seem to have any DNS name it would if we were in Docker land. todo link to docker lab?\n",
    "\n",
    "Isolation? What if I wanted to isolate my pods? todo https://kubernetes.io/docs/concepts/services-networking/network-policies/\n",
    "\n",
    "# Introducing Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo tailscale cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pod.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pod.yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: iperf3\n",
    "    labels:\n",
    "      app: iperf3\n",
    "spec:\n",
    "    containers:\n",
    "      - name: iperf3\n",
    "        image: networkstatic/iperf3\n",
    "        args:\n",
    "          - --server\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: caddy\n",
    "    labels:\n",
    "      app: caddy\n",
    "spec:\n",
    "    containers:\n",
    "      - name: caddy\n",
    "        image: caddy/caddy\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "    name: caddy\n",
    "spec:\n",
    "    selector:\n",
    "        app: caddy\n",
    "    ports:\n",
    "      - protocol: TCP\n",
    "        port: 80\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "    name: iperf3\n",
    "spec:\n",
    "    selector:\n",
    "        app: iperf3\n",
    "    ports:\n",
    "      - protocol: TCP\n",
    "        port: 5201\n",
    "        # todo does iperf also need UDP?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "I just added the labels `app: caddy` and `app: iperf3` to the respective pods. In the Service, I am using those labels to target my apps.\n",
    "\n",
    "Think this looks complex? Think again! Check out my lab on container-container networking in Docker Compose. I haven't created it yet but I want to! This [Service](https://kubernetes.io/docs/concepts/services-networking/service/) definition may seem complex, but so is todo.\n",
    "\n",
    "Think of a service as an exposed port in Docker. The ser actually not really\n",
    "\n",
    "This medium article on [Mastering Kubernetes Pod-to-Pod Communication](https://medium.com/@extio/mastering-kubernetes-pod-to-pod-communication-a-comprehensive-guide-46832b30556b) was also great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/iperf3 unchanged\n",
      "pod/caddy unchanged\n",
      "service/caddy created\n",
      "service/iperf3 created\n"
     ]
    }
   ],
   "source": [
    "! kubectl apply -f pod.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to caddy-service:80 (10.96.43.142:80)\n",
      "saving to 'index.html'\n",
      "index.html           100% |********************************| 12256  0:00:00 ETA\n",
      "'index.html' saved\n"
     ]
    }
   ],
   "source": [
    "! kubectl exec tailscale -- wget caddy-service:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (7) Failed to connect to tailscale port 80 after 488 ms: Couldn't connect to server\n"
     ]
    }
   ],
   "source": [
    "! curl tailscale:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "It works! But as expected, we still can't reach Caddy by connecting directly to Tailscale.\n",
    "\n",
    "Just like in the Docker lab, there are two solutions:\n",
    "The first is to configure Tailscale act as a reverse proxy to Caddy. Since we can reach Tailscale and Tailscale can reach Caddy, we can have Tailscale reverse proxy our HTTP requests to Caddy for us.\n",
    "\n",
    "However, this would only work for one service at a time, and at that point we might as well leave Tailscale in the Caddy pod. I want to be able to access iperf3 and Caddy. I could run two Tailscales in each pod, but I'd rather have one Tailscale instance act as a VPN for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\n",
      "caddy    ClusterIP   10.96.103.205   <none>        80/TCP     35s\n",
      "iperf3   ClusterIP   10.96.141.159   <none>        5201/TCP   34s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get services caddy iperf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pod.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pod.yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "    name: tailscale\n",
    "spec:\n",
    "    containers:\n",
    "      - name: tailscale\n",
    "        image: ghcr.io/tailscale/tailscale:latest\n",
    "        env:\n",
    "          # disable Kubernetes secrets and use OAuth login instead\n",
    "          - name: TS_KUBE_SECRET\n",
    "            value: \"\"\n",
    "          - name: TS_STATE_DIR\n",
    "            value: \"/var/lib/tailscale\"\n",
    "          - name: TS_USERSPACE\n",
    "            value: \"false\"\n",
    "          - name: TS_DEST_IP\n",
    "            value: \"10.96.0.0/16\"\n",
    "        volumeMounts:\n",
    "          - name: tailscale-state\n",
    "            mountPath: \"/var/lib/tailscale\"\n",
    "        securityContext:\n",
    "          capabilities:\n",
    "            add:\n",
    "              - NET_ADMIN\n",
    "    volumes:\n",
    "      - name: tailscale-state\n",
    "        persistentVolumeClaim:\n",
    "          claimName: tailscale-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: error parsing pod.yaml: error converting YAML to JSON: yaml: line 21: did not find expected '-' indicator\n"
     ]
    }
   ],
   "source": [
    "! kubectl apply -f pod.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/1w5v338qk5m8khcazwclprs3znqp6f7f-python-3.10.7/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
